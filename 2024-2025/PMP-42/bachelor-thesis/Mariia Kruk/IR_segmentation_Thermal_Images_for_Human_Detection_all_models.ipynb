{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from torch.amp import GradScaler, autocast\n",
        "import gc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if not torch.cuda.is_available():\n",
        "    logging.warning(\"CUDA is not available. Running on CPU, which will be slow.\")\n",
        "    print(\"CUDA is not available. Running on CPU, which will be slow.\")\n",
        "\n",
        "files.upload()\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d sikdermdsaiful/thermal-images-for-human-detection -p /content\n",
        "!unzip /content/thermal-images-for-human-detection.zip -d /content/thermal-images-for-human-detection"
      ],
      "metadata": {
        "id": "naqI2lLuH47T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "!pip install opencv-python numpy matplotlib scikit-learn torch torchvision albumentations scipy pydensecrf tqdm segmentation-models-pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax"
      ],
      "metadata": {
        "id": "kO8FDX25IQPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(SegNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KK7WRivuWZMo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HRNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(HRNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.out = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eIsGhqc0I6EZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_infrared(image, image_path=\"unknown\"):\n",
        "    image = image.astype(np.float32)\n",
        "    min_val, max_val = np.min(image), np.max(image)\n",
        "    print(f\"Raw image min/max for {image_path}: {min_val}, {max_val}\")\n",
        "    if np.any(np.isnan(image)) or np.any(np.isinf(image)):\n",
        "        print(f\"Warning: NaN or Inf in raw image {image_path}\")\n",
        "        return None\n",
        "\n",
        "    image = cv2.medianBlur(image, 3)\n",
        "\n",
        "    min_val = np.percentile(image, 2)\n",
        "    max_val = np.percentile(image, 98)\n",
        "    if max_val <= min_val:\n",
        "        print(f\"Warning: Invalid min/max values in {image_path}: min={min_val}, max={max_val}\")\n",
        "        return None\n",
        "    image = (image - min_val) / (max_val - min_val + 1e-7)\n",
        "    image = np.clip(image, 0, 1)\n",
        "\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    image = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8,8)).apply(image)\n",
        "\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    image = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "\n",
        "    if np.any(np.isnan(image)) or np.any(np.isinf(image)):\n",
        "        print(f\"Warning: NaN or Inf in preprocessed image {image_path}\")\n",
        "        return None\n",
        "\n",
        "    image = np.stack([image, image, image], axis=-1)\n",
        "    return image.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "sfwKHQcKWd5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def apply_imagenet_normalization(image):\n",
        "    image = image.transpose(2, 0, 1)\n",
        "    image = torch.from_numpy(image).float()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    image = (image - mean) / std\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    return image.astype(np.float32)\n",
        "\n",
        "def load_image_and_mask(image_path, label_path, num_classes=1, target_size=(224, 224)):\n",
        "    try:\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Failed to load {image_path}\")\n",
        "            return None, None, None\n",
        "\n",
        "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "        image = preprocess_infrared(image, image_path)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Preprocessing failed for {image_path}\")\n",
        "            return None, None, None\n",
        "\n",
        "        mask = np.zeros((target_size[0], target_size[1]), dtype=np.float32)\n",
        "        class_counts = {'Human': 0}\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\"Warning: Annotation {label_path} does not exist\")\n",
        "            return None, None, None\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            annotations = f.readlines()\n",
        "\n",
        "        print(f\"Image {image_path}: {len(annotations)} annotations\")\n",
        "        for ann in annotations:\n",
        "            parts = ann.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                print(f\"Invalid annotation format in {label_path}: {ann}\")\n",
        "                continue\n",
        "\n",
        "            class_id, x_center, y_center, width, height = map(float, parts)\n",
        "            class_id = int(class_id)\n",
        "\n",
        "            x_center = x_center * target_size[1]\n",
        "            y_center = y_center * target_size[0]\n",
        "            width = max(width * target_size[1] * 1.5, 30)\n",
        "            height = max(height * target_size[0] * 1.5, 30)\n",
        "\n",
        "            x1 = int(x_center - width / 2)\n",
        "            y1 = int(y_center - height / 2)\n",
        "            x2 = int(x_center + width / 2)\n",
        "            y2 = int(y_center + height / 2)\n",
        "\n",
        "            x2 = max(x2, x1 + 30)\n",
        "            y2 = max(y2, y1 + 30)\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(target_size[1]-1, x2), min(target_size[0]-1, y2)\n",
        "\n",
        "            if x1 >= x2 or y1 >= y2:\n",
        "                print(f\"Invalid bounding box in {image_path}: ({x1}, {y1}, {x2}, {y2})\")\n",
        "                continue\n",
        "\n",
        "            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
        "            class_counts['Human'] += 1\n",
        "\n",
        "        mask = mask / 255.0\n",
        "        mask_sum = mask.sum()\n",
        "        print(f\"Image {image_path}: Mask pixel sum before resize: {mask_sum}\")\n",
        "        if mask_sum < 100:\n",
        "            print(f\"Warning: Empty or nearly empty mask for {image_path} (sum={mask_sum})\")\n",
        "            return None, None, None\n",
        "\n",
        "        return image, mask, class_counts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {str(e)}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "ngo5VNBKWgIm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/thermal-images-for-human-detection/dataset\"\n",
        "train_image_dir = os.path.join(data_dir, \"train\", \"images\")\n",
        "train_label_dir = os.path.join(data_dir, \"train\", \"labels\")\n",
        "test_image_dir = os.path.join(data_dir, \"test\", \"images\")\n",
        "test_label_dir = os.path.join(data_dir, \"test\", \"labels\")\n",
        "\n",
        "for directory in [train_image_dir, train_label_dir, test_image_dir, test_label_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Error: Directory {directory} does not exist\")\n",
        "        raise FileNotFoundError(f\"Directory {directory} not found\")\n",
        "\n",
        "train_image_files = sorted([f for f in os.listdir(train_image_dir) if f.endswith('.jpg')])\n",
        "train_label_files = sorted([f for f in os.listdir(train_label_dir) if f.endswith('.txt')])\n",
        "test_image_files = sorted([f for f in os.listdir(test_image_dir) if f.endswith('.jpg')])\n",
        "test_label_files = sorted([f for f in os.listdir(test_label_dir) if f.endswith('.txt')])\n",
        "\n",
        "print(f\"Found {len(train_image_files)} training images and {len(train_label_files)} training labels\")\n",
        "print(f\"Found {len(test_image_files)} test images and {len(test_label_files)} test labels\")\n",
        "\n",
        "train_image_files = train_image_files[:600]\n",
        "print(f\"Using {len(train_image_files)} images for training\")\n",
        "\n",
        "images, masks, image_labels = [], [], []\n",
        "all_class_counts = {'Human': 0}\n",
        "skipped_images = 0\n",
        "\n",
        "for img_file in train_image_files:\n",
        "    img_path = os.path.join(train_image_dir, img_file)\n",
        "    label_file = img_file.replace('.jpg', '.txt')\n",
        "    label_path = os.path.join(train_label_dir, label_file)\n",
        "\n",
        "    image, mask, class_counts = load_image_and_mask(img_path, label_path, num_classes=1)\n",
        "    if image is None or mask is None or class_counts is None:\n",
        "        print(f\"Failed to load image: {img_path}\")\n",
        "        skipped_images += 1\n",
        "        continue\n",
        "\n",
        "    mask_sum_before = mask.sum()\n",
        "    mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "    mask_sum_after = mask.sum()\n",
        "\n",
        "    images.append(image)\n",
        "    masks.append(mask)\n",
        "    image_labels.append(class_counts)\n",
        "    for class_name, count in class_counts.items():\n",
        "        all_class_counts[class_name] = all_class_counts.get(class_name, 0) + count\n",
        "\n",
        "\n",
        "avg_foreground_pixels = np.mean(np.array(masks).sum(axis=(1, 2)))\n",
        "print(f\"Average foreground pixels per mask: {avg_foreground_pixels:.2f}\")\n",
        "print(f\"Overall class distribution: {all_class_counts}\")\n",
        "\n",
        "target_size = (224, 224)\n",
        "images = np.array(images, dtype=np.float32)\n",
        "masks = np.array(masks, dtype=np.float32)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(min(3, len(images))):\n",
        "    raw_image = cv2.imread(os.path.join(train_image_dir, train_image_files[i]), cv2.IMREAD_GRAYSCALE)\n",
        "    raw_image = cv2.resize(raw_image, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "    plt.subplot(3, 3, i*3 + 1)\n",
        "    plt.imshow(raw_image, cmap='gray')\n",
        "    plt.title(f'Raw Image {i+1}')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(3, 3, i*3 + 2)\n",
        "    plt.imshow(images[i][:, :, 0], cmap='gray')\n",
        "    plt.title(f'Processed Image {i+1}')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(3, 3, i*3 + 3)\n",
        "    plt.imshow(masks[i], cmap='gray')\n",
        "    plt.title(f'Mask (Sample {i+1})')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TwITvyazWlmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmenter():\n",
        "    return A.Compose([\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Rotate(limit=60, p=0.8),\n",
        "        A.RandomScale(scale_limit=0.3, p=0.6),\n",
        "        A.Affine(translate_percent=0.2, scale=(0.8, 1.2), rotate=(-45, 45), p=0.6),\n",
        "        A.RandomGamma(gamma_limit=(90, 110), p=0.5),\n",
        "        A.Resize(height=224, width=224, p=1.0)\n",
        "    ], additional_targets={'mask': 'mask'})\n",
        "\n",
        "augmenter = get_augmenter()\n",
        "aug_images, aug_masks, aug_labels = [], [], []\n",
        "augmentation_factor = 3\n",
        "\n",
        "for img_idx, (img, mask, label_counts) in enumerate(zip(images, masks, image_labels)):\n",
        "    aug_images.append(apply_imagenet_normalization(img))\n",
        "    aug_masks.append(mask)\n",
        "    aug_labels.append(label_counts)\n",
        "    for aug_iter in range(augmentation_factor):\n",
        "        try:\n",
        "            img_clipped = np.clip(img, 0, 1)\n",
        "            mask_clipped = np.clip(mask, 0, 1)\n",
        "            augmented = augmenter(image=img_clipped, mask=mask_clipped)\n",
        "            aug_img = augmented['image']\n",
        "            aug_mask = augmented['mask']\n",
        "\n",
        "            if aug_img.shape != (224, 224, 3) or aug_mask.shape != (224, 224):\n",
        "                print(f\"Invalid shape after augmentation for image {img_idx}: image={aug_img.shape}, mask={aug_mask.shape}\")\n",
        "                continue\n",
        "            if np.any(np.isnan(aug_img)) or np.any(np.isinf(aug_img)) or np.any(np.isnan(aug_mask)) or np.any(np.isinf(aug_mask)):\n",
        "                print(f\"Warning: NaN or Inf in augmented data for image {img_idx}, iteration {aug_iter}\")\n",
        "                continue\n",
        "\n",
        "            aug_img = np.clip(aug_img, 0, 1)\n",
        "            aug_img = apply_imagenet_normalization(aug_img)\n",
        "            aug_images.append(aug_img)\n",
        "            aug_masks.append(aug_mask)\n",
        "            aug_labels.append(label_counts)\n",
        "        except Exception as e:\n",
        "            print(f\"Augmentation failed for image {img_idx}, iteration {aug_iter}: {e}\")\n",
        "            continue\n",
        "\n",
        "aug_images = np.array(aug_images, dtype=np.float32)\n",
        "aug_masks = np.array(aug_masks, dtype=np.float32)\n",
        "\n",
        "print(\"Augmented images shape:\", aug_images.shape)\n",
        "print(\"Augmented masks shape:\", aug_masks.shape)\n",
        "print(f\"Augmented mask pixel counts: {aug_masks.sum(axis=(1, 2))}\")\n",
        "print(f\"Number of labels: {len(aug_labels)}\")\n",
        "\n",
        "all_images = aug_images\n",
        "all_masks = aug_masks\n",
        "all_labels = aug_labels\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def get_multi_class_labels(labels, categories):\n",
        "    multi_labels = []\n",
        "    for label_counts in labels:\n",
        "        multi_label = [0] * len(categories)\n",
        "        for idx, cat in enumerate(categories):\n",
        "            if label_counts.get(cat, 0) > 0:\n",
        "                multi_label[idx] = 1\n",
        "        multi_labels.append(multi_label)\n",
        "    return np.array(multi_labels)\n",
        "\n",
        "categories = ['Human']\n",
        "multi_class_labels = get_multi_class_labels(all_labels, categories)\n",
        "X_train, X_val, y_train, y_val, train_labels, val_labels = train_test_split(\n",
        "    all_images,\n",
        "    np.expand_dims(all_masks, axis=-1),\n",
        "    multi_class_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=multi_class_labels\n",
        ")\n",
        "\n",
        "print(f\"Train images min/max: {np.min(X_train)}, {np.max(X_train)}\")\n",
        "print(f\"Train masks min/max: {np.min(y_train)}, {np.max(y_train)}\")\n",
        "print(f\"Val images min/max: {np.min(X_val)}, {np.max(X_val)}\")\n",
        "print(f\"Val masks min/max: {np.min(y_val)}, {np.max(y_val)}\")\n",
        "print(f\"Train mask pixel sums: {y_train.sum(axis=(1, 2, 3))}\")\n",
        "print(f\"Val mask pixel sums: {y_val.sum(axis=(1, 2, 3))}\")\n",
        "\n",
        "val_class_presence = {cat: 0 for cat in categories}\n",
        "for labels in val_labels:\n",
        "    for idx, present in enumerate(labels):\n",
        "        if present == 1:\n",
        "            val_class_presence[categories[idx]] += 1\n",
        "print(f\"Validation set class presence: {val_class_presence}\")\n",
        "\n",
        "print(f\"Train dataset size: {len(X_train)}, Val dataset size: {len(X_val)}\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"Class pixel counts: {y_train.sum()}\")\n",
        "\n",
        "assert all(img.shape == (224, 224, 3) for img in X_train), \"Invalid image shape in X_train\"\n",
        "assert all(mask.shape == (224, 224, 1) for mask in y_train), \"Invalid mask shape in y_train\"\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(min(2, len(X_train))):\n",
        "    plt.subplot(2, 2, i * 2 + 1)\n",
        "    plt.imshow(X_train[i][:, :, 0], cmap='gray')\n",
        "    plt.title(f'Train Image {i+1}')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(2, 2, i * 2 + 2)\n",
        "    plt.imshow(y_train[i].squeeze(), cmap='gray')\n",
        "    plt.title(f'Train Mask')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(min(2, len(X_val))):\n",
        "    plt.subplot(2, 2, i * 2 + 1)\n",
        "    plt.imshow(X_val[i][:, :, 0], cmap='gray')\n",
        "    plt.title(f'Val Image {i+1}')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(2, 2, i * 2 + 2)\n",
        "    plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
        "    plt.title(f'Val Mask')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Al2HlqmoWp4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1e-7):\n",
        "    y_true = y_true.view(y_true.size(0), y_true.size(1), -1)\n",
        "    y_pred = y_pred.view(y_pred.size(0), y_pred.size(1), -1)\n",
        "    intersection = (y_true * y_pred).sum(dim=2)\n",
        "    dice = (2. * intersection + smooth) / (y_true.sum(dim=2) + y_pred.sum(dim=2) + smooth)\n",
        "    return dice.mean()\n",
        "\n",
        "def jaccard_index(y_true, y_pred, smooth=1e-7):\n",
        "    y_true = y_true.view(y_true.size(0), y_pred.size(1), -1)\n",
        "    y_pred = y_pred.view(y_true.size(0), y_pred.size(1), -1)\n",
        "    intersection = (y_true * y_pred).sum(dim=2)\n",
        "    union = y_true.sum(dim=2) + y_pred.sum(dim=2) - intersection\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou.mean()\n",
        "\n",
        "def pixel_accuracy(y_true, y_pred):\n",
        "    y_true_classes = (y_true > 0.5).float()\n",
        "    y_pred_classes = (y_pred > 0.5).float()\n",
        "    correct = (y_true_classes == y_pred_classes).float()\n",
        "    return correct.mean()\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-7, label_smoothing=0.05):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        if y_true.sum() == 0:\n",
        "            return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
        "        y_pred = torch.clamp(y_pred, -1e5, 1e5)\n",
        "        y_pred = torch.sigmoid(y_pred)\n",
        "        y_true = y_true * (1 - 2 * self.label_smoothing) + self.label_smoothing\n",
        "        y_true = y_true.view(y_true.size(0), y_true.size(1), -1)\n",
        "        y_pred = y_pred.view(y_pred.size(0), y_pred.size(1), -1)\n",
        "        intersection = (y_true * y_pred).sum(dim=2)\n",
        "        dice = (2. * intersection + self.smooth) / (y_true.sum(dim=2) + y_pred.sum(dim=2) + self.smooth)\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, label_smoothing=0.05):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        if y_true.sum() == 0:\n",
        "            return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
        "        y_pred = torch.clamp(y_pred, -1e5, 1e5)\n",
        "        y_true = y_true * (1 - 2 * self.label_smoothing) + self.label_smoothing\n",
        "        bce = F.binary_cross_entropy_with_logits(y_pred, y_true, reduction='none')\n",
        "        pt = torch.exp(-bce)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
        "        return focal_loss.mean()\n"
      ],
      "metadata": {
        "id": "IPxnv0r3WtXS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Unet\"\n",
        "model_configs = {\n",
        "    \"DeepLabV3Plus\": {\n",
        "        \"class\": smp.DeepLabV3Plus,\n",
        "        \"save_prefix\": \"deeplabv3plus_hit_uav\",\n",
        "        \"params\": {\"encoder_name\": \"resnet50\", \"in_channels\": 3, \"classes\": 1}\n",
        "    },\n",
        "    \"Unet\": {\n",
        "        \"class\": smp.Unet,\n",
        "        \"save_prefix\": \"unet_hit_uav\",\n",
        "        \"params\": {\"encoder_name\": \"resnet50\", \"in_channels\": 3, \"classes\": 1}\n",
        "    },\n",
        "    \"PSPNet\": {\n",
        "        \"class\": smp.PSPNet,\n",
        "        \"save_prefix\": \"pspnet_hit_uav\",\n",
        "        \"params\": {\"encoder_name\": \"resnet50\", \"in_channels\": 3, \"classes\": 1}\n",
        "    },\n",
        "    \"SegNet\": {\n",
        "        \"class\": SegNet,\n",
        "        \"save_prefix\": \"segnet_hit_uav\",\n",
        "        \"params\": {\"in_channels\": 3, \"out_channels\": 1}\n",
        "    },\n",
        "    \"HRNet\": {\n",
        "        \"class\": HRNet,\n",
        "        \"save_prefix\": \"hrnet_hit_uav\",\n",
        "        \"params\": {\"in_channels\": 3, \"out_channels\": 1}\n",
        "    }\n",
        "}\n",
        "\n",
        "if model_name not in model_configs:\n",
        "    raise ValueError(f\"Model {model_name} is not supported. Choose from {list(model_configs.keys())}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "PaxdDjAtJCPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_config = model_configs[model_name]\n",
        "model_class = model_config[\"class\"]\n",
        "model_params = model_config[\"params\"]\n",
        "model = model_class(**model_params).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "model.apply(init_weights)\n",
        "\n",
        "X_train_torch = torch.from_numpy(X_train).permute(0, 3, 1, 2).float().to(device)\n",
        "y_train_torch = torch.from_numpy(y_train).permute(0, 3, 1, 2).float().to(device)\n",
        "X_val_torch = torch.from_numpy(X_val).permute(0, 3, 1, 2).float().to(device)\n",
        "y_val_torch = torch.from_numpy(y_val).permute(0, 3, 1, 2).float().to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
        "\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "print(f\"Dataset size: {len(train_dataset)}, Batches: {len(train_loader)}\")\n",
        "\n",
        "all_masks = np.concatenate([y_train, y_val], axis=0)\n",
        "foreground_pixels = all_masks.sum() / (all_masks.size * all_masks.shape[-2] * all_masks.shape[-1])\n",
        "background_pixels = 1 - foreground_pixels\n",
        "pos_weight = torch.tensor([min(background_pixels / (foreground_pixels + 1e-7), 50.0)]).to(device)\n",
        "print(f\"Pos weight for BCE: {pos_weight.item()}\")\n",
        "\n",
        "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "dice_loss = DiceLoss(smooth=1e-7, label_smoothing=0.05)\n",
        "focal_loss = FocalLoss(alpha=1.0, gamma=2.0, label_smoothing=0.05)\n",
        "\n",
        "def combined_loss(y_pred, y_true, bce_weight=0.1, dice_weight=0.5, phase=\"train\"):\n",
        "    if y_true.sum() == 0:\n",
        "        print(f\"Warning: Empty mask in {phase} batch, skipping loss\")\n",
        "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
        "    bce = bce_loss(y_pred, y_true)\n",
        "    dice = dice_loss(y_pred, y_true)\n",
        "    focal = focal_loss(y_pred, y_true)\n",
        "    if torch.isnan(bce) or torch.isinf(bce) or torch.isnan(dice) or torch.isinf(dice) or torch.isnan(focal) or torch.isinf(focal):\n",
        "        print(f\"Warning: Invalid loss components in {phase} - BCE: {bce.item()}, Dice: {dice.item()}, Focal: {focal.item()}\")\n",
        "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
        "    total_loss = bce_weight * bce + dice_weight * dice + (1 - bce_weight - dice_weight) * focal\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "6S6ZW5MiJFlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7)\n",
        "\n",
        "num_epochs = 50\n",
        "warmup_epochs = 5\n",
        "base_lr = 3e-4\n",
        "warmup_lr = 3e-5\n",
        "best_val_iou = 0.0\n",
        "patience = 15\n",
        "epochs_no_improve = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss, train_iou = 0.0, 0.0\n",
        "\n",
        "    if epoch < warmup_epochs:\n",
        "        lr = warmup_lr + (base_lr - warmup_lr) * (epoch / warmup_epochs)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    try:\n",
        "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            if isinstance(outputs, dict):\n",
        "                outputs = outputs['out']\n",
        "            if epoch == 0 and batch_idx == 0:\n",
        "                print(f\"Outputs shape: {outputs.shape}, Masks shape: {masks.shape}\")\n",
        "\n",
        "            loss = combined_loss(outputs, masks, phase=\"train\")\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"Warning: Invalid loss at epoch {epoch+1}, batch {batch_idx}: {loss.item()}\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            iou = jaccard_index(masks, preds)\n",
        "            train_iou += iou.item() * images.size(0)\n",
        "    except Exception as e:\n",
        "        print(f\"DataLoader error: {e}\")\n",
        "        raise\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_iou /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_iou = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            if isinstance(outputs, dict):\n",
        "                outputs = outputs['out']\n",
        "            loss = combined_loss(outputs, masks, phase=\"val\")\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            iou = jaccard_index(masks, preds)\n",
        "            val_iou += iou.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_iou /= len(val_loader.dataset)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_iou'].append(train_iou)\n",
        "    history['val_iou'].append(val_iou)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "          f\"Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_iou > best_val_iou:\n",
        "        best_val_iou = val_iou\n",
        "        torch.save(model.state_dict(), f\"{model_config['save_prefix']}_thermal_images_human_detection.pth\")\n",
        "        print(f\"New best model saved at epoch {epoch+1} with Val IoU: {val_iou:.4f}\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "MwgwIRyGWxdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(preds):\n",
        "    preds = torch.sigmoid(preds)\n",
        "    return preds.cpu().numpy()\n",
        "\n",
        "model.eval()\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        if isinstance(outputs, dict):\n",
        "            outputs = outputs['out']\n",
        "        y_pred.append(outputs.cpu())\n",
        "y_pred = torch.cat(y_pred, dim=0)\n",
        "y_pred = postprocess(y_pred)\n",
        "\n",
        "y_pred_torch = torch.from_numpy(y_pred).float().to(device)\n",
        "y_val_torch = y_val_torch.to(device)\n",
        "val_iou = jaccard_index(y_val_torch, y_pred_torch)\n",
        "val_dice = dice_coefficient(y_val_torch, y_pred_torch)\n",
        "val_accuracy = pixel_accuracy(y_val_torch, y_pred_torch)\n",
        "\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(f\"Jaccard Index: {val_iou:.4f}\")\n",
        "print(f\"Dice Coefficient: {val_dice:.4f}\")\n",
        "print(f\"Pixel Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(min(3, len(X_val))):\n",
        "    plt.subplot(3, 2, i * 2 + 1)\n",
        "    plt.imshow(X_val[i][:, :, 0], cmap='gray')\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(3, 2, i * 2 + 2)\n",
        "    plt.imshow(y_pred[i].squeeze(), cmap='gray')\n",
        "    plt.title(f'Predicted Mask')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 2)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_iou'], label='Train IoU')\n",
        "plt.plot(history['val_iou'], label='Val IoU')\n",
        "plt.title('IoU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IoU Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dvgkz6uwOswD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}